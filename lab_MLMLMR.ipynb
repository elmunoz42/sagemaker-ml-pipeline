{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker Pipelines and the SageMaker Model Registry with SageMaker Studio\n",
    "\n",
    "In this lab you create and run an Amazon Sagemaker Pipeline and monitor the pipeline's progress. You also locate and explore some of the artifacts that the machine learning (ML) process uses or generates.\n",
    "\n",
    "If time permits, you can also review the lineage details for the model that the pipeline generated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Environment setup\n",
    "\n",
    "Before you create your SageMaker Pipeline, you must prepare the environment by installing necessary packages, importing modules, and staging supporting files. This Sagemaker Pipeline was designed to use a feature group, so you also create a feature group in Amazon SageMaker Feature Store and run a Data Wrangler flow to prepare your environment. \n",
    "\n",
    "Run the cells in this task to do the following:\n",
    "- Install dependencies.\n",
    "- Import required modules.\n",
    "- Copy data and code to Amazon Simple Storage Service (Amazon S3).\n",
    "- Create a feature group.\n",
    "- Ingest features into the feature group."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies\n",
    "%pip install --upgrade pip \n",
    "%pip install pytest-astropy ==  0.7.0\n",
    "%pip install rsa == 4.7.2\n",
    "%pip install PyYAML\n",
    "!apt update && apt install -y git\n",
    "%pip install git+https://github.com/aws-samples/ml-lineage-helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-modules\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import uuid\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThan\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from ml_lineage_helper import *\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition\n",
    "from sagemaker.feature_store.feature_definition import FeatureTypeEnum\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FeatureStoreOutput\n",
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sessions\n",
    "boto_session  =  boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clients\n",
    "s3_client = boto3.client('s3')\n",
    "featurestore_runtime = boto3.client('sagemaker-featurestore-runtime')\n",
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature store session\n",
    "feature_store_session = Session(\n",
    "    boto_session = boto_session,\n",
    "    sagemaker_client = sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client = featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set global variables\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "region = boto_session.region_name\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Copy lab files to Amazon S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to default bucket\n",
    "s3_client.put_object(Bucket = default_bucket, Key = 'data/')\n",
    "s3_client.put_object(Bucket = default_bucket, Key = 'input/code/')\n",
    "s3_client.upload_file('pipelines/data/storedata_total.csv', default_bucket, 'data/storedata_total.csv')\n",
    "s3_client.upload_file('pipelines/input/code/evaluate.py', default_bucket, 'input/code/evaluate.py')\n",
    "s3_client.upload_file('pipelines/input/code/generate_config.py', default_bucket, 'input/code/generate_config.py')\n",
    "s3_client.upload_file('pipelines/input/code/processfeaturestore.py', default_bucket, 'input/code/processfeaturestore.py')\n",
    "\n",
    "# Preview the dataset\n",
    "print('Dataset preview:')\n",
    "customer_data = pd.read_csv('pipelines/data/storedata_total.csv')\n",
    "customer_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Create the feature group"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you create a feature group for the data. First, create a schema of the data. For this lab, the schema should be sorted first by the column **name**, and then by the column **type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up-feature-store-variables\n",
    "record_identifier_feature_name = 'FS_ID'\n",
    "event_time_feature_name = 'FS_time'\n",
    "\n",
    "column_schemas = [\n",
    "    {\n",
    "        \"name\": \"retained\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"esent\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"eopenrate\",\n",
    "        \"type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"eclickrate\",\n",
    "        \"type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"avgorder\",\n",
    "        \"type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ordfreq\",\n",
    "        \"type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"paperless\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"refill\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"doorstep\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"first_last_days_diff\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"created_first_days_diff\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Friday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Monday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Saturday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Sunday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Thursday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Tuesday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"favday_Wednesday\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"city_BLR\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"city_BOM\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"city_DEL\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"city_MAA\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FS_ID\",\n",
    "        \"type\": \"long\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FS_time\",\n",
    "        \"type\": \"float\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow name and a unique ID for this export (used later as the processing job name for the export)\n",
    "flow_name = 'featureengineer'\n",
    "flow_export_id = f\"{strftime('%d-%H-%M-%S', gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "\n",
    "# Feature group name, with flow_name and a unique id. You can give it a customized name\n",
    "feature_group_name = f\"FG-{flow_name}-{str(uuid.uuid4())[:8]}\"\n",
    "\n",
    "# SageMaker Feature Store writes the data in the offline store of a Feature Group to a \n",
    "# Amazon S3 location owned by you.\n",
    "feature_store_offline_s3_uri = 's3://' + default_bucket\n",
    "\n",
    "# Controls if online store is enabled. Enabling the online store allows quick access to \n",
    "# the latest value for a record by using the GetRecord API.\n",
    "enable_online_store = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-feature-group\n",
    "default_feature_type = FeatureTypeEnum.STRING\n",
    "column_to_feature_type_mapping = {\n",
    "    \"float\": FeatureTypeEnum.FRACTIONAL,\n",
    "    \"long\": FeatureTypeEnum.INTEGRAL\n",
    "}\n",
    "\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(\n",
    "        feature_name = column_schema['name'], \n",
    "        feature_type = column_to_feature_type_mapping.get(column_schema['type'], default_feature_type)\n",
    "    ) for column_schema in column_schemas\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Feature Group Name: {feature_group_name}\")\n",
    "\n",
    "# Confirm the Athena settings are configured\n",
    "try:\n",
    "    boto3.client('athena').update_work_group(\n",
    "        WorkGroup = 'primary',\n",
    "        ConfigurationUpdates = {\n",
    "            'EnforceWorkGroupConfiguration':False\n",
    "        }\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    name = feature_group_name, sagemaker_session = feature_store_session, feature_definitions = feature_definitions)\n",
    "\n",
    "feature_group.create(\n",
    "    s3_uri = feature_store_offline_s3_uri,\n",
    "    record_identifier_name = record_identifier_feature_name,\n",
    "    event_time_feature_name = event_time_feature_name,\n",
    "    role_arn = role,\n",
    "    enable_online_store = enable_online_store\n",
    ")\n",
    "\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    \"\"\"Helper function to wait for the completions of creating a feature group\"\"\"\n",
    "    response = feature_group.describe()\n",
    "    status = response.get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for feature group creation\")\n",
    "        time.sleep(5)\n",
    "        response = feature_group.describe()\n",
    "        status = response.get(\"FeatureGroupStatus\")\n",
    "\n",
    "    if status != \"Created\":\n",
    "        print(f\"Failed to create feature group, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create feature group {feature_group.name}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"Feature Group {feature_group.name} successfully created.\")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group = feature_group)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Ingest features\n",
    "\n",
    "This process takes approximately 8 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate-feature-store\n",
    "column_list = ['retained','esent','eopenrate','eclickrate','avgorder','ordfreq','paperless','refill','doorstep','first_last_days_diff','created_first_days_diff','favday_Friday','favday_Monday', 'favday_Saturday','favday_Sunday','favday_Thursday','favday_Tuesday','favday_Wednesday','city_BLR','city_BOM','city_DEL','city_MAA','FS_ID','FS_time']\n",
    "lab_test_data = pd.read_csv('featureengineer_data/store_data_processed.csv', names = (column_list), header = 1)\n",
    "feature_group.ingest(data_frame = lab_test_data, wait = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.7: Create and run a SageMaker Pipeline\n",
    "\n",
    "Now that your environment is set up, you configure, create, and start a SageMaker Pipeline. \n",
    "\n",
    "A SageMaker Pipeline is a workflow that runs a set of dependent steps. Steps can accept inputs and send outputs, so data and other assets can be passed between them. \n",
    "\n",
    "Run the code in the next cells to do the following:\n",
    "- Define variables that are needed to configure the pipeline.\n",
    "- Configure a SageMaker session.\n",
    "- Define the pipeline steps.\n",
    "- Configure the pipeline.\n",
    "- Create the pipeline.\n",
    "- Start the pipeline.\n",
    "- Describe the pipeline.\n",
    "- Create a wait event so that the notebook does not proceed until the pipeline has finished running."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.8: Set up the variables that the pipeline uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pipeline-variables\n",
    "feature_group_name = feature_group.name\n",
    "model_name = \"Churn-model\"\n",
    "\n",
    "sklearn_processor_version = \"0.23-1\"\n",
    "model_package_group_name = \"ChurnModelPackageGroup\"\n",
    "pipeline_name = \"ChurnModelSMPipeline\"\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name = \"ProcessingInstanceCount\",\n",
    "    default_value = 1\n",
    "    )\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "        name = \"ProcessingInstanceType\",\n",
    "        default_value = \"ml.m5.xlarge\"\n",
    "    )\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "        name = \"TrainingInstanceType\",\n",
    "        default_value = \"ml.m5.xlarge\"\n",
    "    )\n",
    "\n",
    "input_data = ParameterString(\n",
    "        name = \"InputData\",\n",
    "        default_value = \"s3://{}/data/storedata_total.csv\".format(default_bucket), \n",
    "    )\n",
    "\n",
    "batch_data = ParameterString(\n",
    "        name = \"BatchData\",\n",
    "        default_value = \"s3://{}/data/batch/batch.csv\".format(default_bucket),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.9: Configure the pipeline\n",
    "\n",
    "You define a pipeline named **ChurnModelPipeline** to produce a model that evaluates the likelihood of retaining or losing customers. This pipeline has nine steps. \n",
    "\n",
    "Each step in a pipeline runs a specific job type. The required inputs for a job vary based on the job type. Refer to [Step Types](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#build-and-manage-steps-types) for more information about SageMaker Pipeline step types.\n",
    "\n",
    "Review the code in the following cells to understand how each step was defined:\n",
    "\n",
    "The **ChurnModelProcess** step is defined in the variable named **step_process**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Processing – Processing jobs are defined using the class ProcessingStep().\n",
    "- **Processor:** SKLearnProcessor.\n",
    "- **Destination:** Output will be sent to folders under your default S3 bucket.\n",
    "- **Job Arguments:** This step will use the Feature Store to process the dataset.\n",
    "- **Code:** **processfeaturestore.py**, which resides in your default S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-processing-step\n",
    "# Run a scikit-learn script to do data processing on SageMaker \n",
    "# using the SKLearnProcessor class\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "        framework_version = sklearn_processor_version,\n",
    "        instance_type = processing_instance_type.default_value, \n",
    "        instance_count = processing_instance_count,\n",
    "        sagemaker_session = sagemaker_session,\n",
    "        role = role,\n",
    "    )\n",
    "\n",
    "# Inputs, outputs, and code are parameters to the processor\n",
    "# step_* will become the pipeline steps toward the end of the cell\n",
    "# in this case, use the feature store as input, so there is no externalinput\n",
    "step_process = ProcessingStep(\n",
    "        name = \"ChurnModelProcess\",\n",
    "        processor = sklearn_processor,\n",
    "        outputs = [\n",
    "            ProcessingOutput(output_name = \"train\", source = \"/opt/ml/processing/train\",\\\n",
    "                             destination = f\"s3://{default_bucket}/output/train\" ),\n",
    "            ProcessingOutput(output_name = \"validation\", source = \"/opt/ml/processing/validation\",\\\n",
    "                            destination = f\"s3://{default_bucket}/output/validation\"),\n",
    "            ProcessingOutput(output_name = \"test\", source = \"/opt/ml/processing/test\",\\\n",
    "                            destination = f\"s3://{default_bucket}/output/test\"),\n",
    "            ProcessingOutput(output_name = \"batch\", source = \"/opt/ml/processing/batch\",\\\n",
    "                            destination = f\"s3://{default_bucket}/data/batch\"),\n",
    "            ProcessingOutput(output_name = \"baseline\", source = \"/opt/ml/processing/baseline\",\\\n",
    "                            destination = f\"s3://{default_bucket}/input/baseline\")\n",
    "        ],\n",
    "        job_arguments = [\"--featuregroupname\",feature_group_name,\"--default-bucket\",default_bucket,\"--region\",region],\n",
    "        code = f\"s3://{default_bucket}/input/code/processfeaturestore.py\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChurnHyperParameterTuning** step is defined in the variable named **step_tuning**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Tuning – Tuning jobs are defined using the class TuningStep().\n",
    "- **Tuner:** This job uses the XGBoost framework.\n",
    "- **Inputs:** Notice that this job uses the training and validation data that was produced by the ChurnModelProcess step named **step_process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-churn-hyperparameter-tuning\n",
    "# Training/tuning step for generating model artifacts\n",
    "model_path = f\"s3://{default_bucket}/output\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework = \"xgboost\",\n",
    "    region = region,\n",
    "    version = \"1.5-1\",\n",
    "    py_version = \"py3\",\n",
    "    instance_type = training_instance_type.default_value,\n",
    ")\n",
    "\n",
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\":\"auc\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\",\n",
    "    \"rate_drop\":\"0.3\",\n",
    "    \"tweedie_variance_power\":\"1.4\"\n",
    "    }\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri = image_uri,\n",
    "    instance_type = training_instance_type,\n",
    "    instance_count = 1,\n",
    "    hyperparameters = fixed_hyperparameters,\n",
    "    output_path = model_path,\n",
    "    base_job_name = f\"churn-train\",\n",
    "    sagemaker_session = sagemaker_session,\n",
    "    role = role\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning steps\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    }\n",
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name = \"ChurnHyperParameterTuning\",\n",
    "    tuner = HyperparameterTuner(xgb_train, objective_metric_name, hyperparameter_ranges, max_jobs = 2, max_parallel_jobs = 2),\n",
    "    inputs = {\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data = step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type = \"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data = step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type = \"text/csv\",\n",
    "            ),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChurnEvalBestModel** step is defined in the variable named **step_eval**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Processing.\n",
    "- **Processor:** ScriptProcessor.\n",
    "- **Inputs:** Notice that this job uses the top model from ChurnHyperParameterTuning (**step_tuning**) and the test output from ChurnModelProcess (**step_process**).\n",
    "- **Outputs:** Output is written to the default S3 bucket.\n",
    "- **Code:** A script named **evaluate.py**, which resides in Amazon S3, is used for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-churn-best-model\n",
    "evaluation_report = PropertyFile(\n",
    "    name = \"ChurnEvaluationReport\",\n",
    "    output_name = \"evaluation\",\n",
    "    path = \"evaluation.json\",\n",
    ")\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri = image_uri,\n",
    "    command = [\"python3\"],\n",
    "    instance_type = processing_instance_type,\n",
    "    instance_count = 1,\n",
    "    base_job_name = \"script-churn-eval\",\n",
    "    role = role,\n",
    "    sagemaker_session = sagemaker_session,\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name = \"ChurnEvalBestModel\",\n",
    "    processor = script_eval,\n",
    "    inputs = [\n",
    "        ProcessingInput(\n",
    "            source = step_tuning.get_top_model_s3_uri(top_k = 0, s3_bucket = default_bucket, prefix = \"output\"),\n",
    "            destination = \"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source = step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination = \"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name = \"evaluation\", source = \"/opt/ml/processing/evaluation\",\\\n",
    "                            destination = f\"s3://{default_bucket}/output/evaluation\"),\n",
    "    ],\n",
    "    code = f\"s3://{default_bucket}/input/code/evaluate.py\",\n",
    "    property_files = [evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChurnCreateModel** step is defined in the variable named **step_create_model**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Model – Model jobs are defined using the class Model().\n",
    "- **Model:** The model used by the step is defined in the previously defined variable named **model**. Notice that the **model** variable uses the top model that was created by ChurnHyperParameterTuning (**step_tuning**).\n",
    "- **Inputs:** The inputs include an instance type and an accelerator type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-model-creation\n",
    "model = Model(\n",
    "    image_uri = image_uri,        \n",
    "    model_data = step_tuning.get_top_model_s3_uri(top_k = 0,s3_bucket = default_bucket,prefix = \"output\"),\n",
    "    name = model_name,\n",
    "    sagemaker_session = sagemaker_session,\n",
    "    role = role,\n",
    ")\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type = \"ml.m5.large\",\n",
    "    accelerator_type = \"ml.inf1.xlarge\",\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name = \"ChurnCreateModel\",\n",
    "    model = model,\n",
    "    inputs = inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChurnModelConfigFile** step is defined in the variable named **step_config_file**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Processing.\n",
    "- **Processor:** ScriptProcessor.\n",
    "- **Code:** **generate_config.py**, which resides in your default S3 bucket.\n",
    "- **Job Arguments:** Job arguments include the model that was generated by **ChurnCreateModel**, the path to the bias report, the default bucket, the number of samples, and the number of instances used for processing.\n",
    "- **Depends On:** Notice that this job cannot run until the model creation has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#configure-script-processing\n",
    "bias_report_output_path = f\"s3://{default_bucket}/clarify-output/bias\"\n",
    "clarify_instance_type = 'ml.m5.xlarge'\n",
    "analysis_config_path = f\"s3://{default_bucket}/clarify-output/bias/analysis_config.json\"\n",
    "clarify_image = sagemaker.image_uris.retrieve(framework = 'sklearn', version = sklearn_processor_version, region = region)\n",
    "\n",
    "#custom_image_uri = None\n",
    "script_processor = ScriptProcessor(\n",
    "    command = ['python3'],\n",
    "    image_uri = clarify_image,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = processing_instance_type,\n",
    "    sagemaker_session = sagemaker_session,\n",
    ")\n",
    "\n",
    "step_config_file = ProcessingStep(\n",
    "    name = \"ChurnModelConfigFile\",\n",
    "    processor = script_processor,\n",
    "    code = f\"s3://{default_bucket}/input/code/generate_config.py\",\n",
    "    job_arguments = [\"--modelname\", step_create_model.properties.ModelName, \"--bias-report-output-path\", bias_report_output_path, \"--clarify-instance-type\", clarify_instance_type,\\\n",
    "                  \"--default-bucket\", default_bucket, \"--num-baseline-samples\", \"50\", \"--instance-count\", \"1\"],\n",
    "    depends_on = [step_create_model.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChurnTransform** step is defined in the variable named **step_transform**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Transform – Transform jobs are defined using the class TransformStep().\n",
    "- **Transformer:** The transformer details are set in the previously defined variable named **transformer**. Notice that this variable is using the model that was created in ChurnCreateModel (**step_create_model**).\n",
    "- **Inputs:** The data that will be transformed, batch.csv, which was defined earlier in the notebook. The input also includes the file type and how it should be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-inference\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    assemble_with = \"Line\",\n",
    "    accept = \"text/csv\",    \n",
    "    output_path = f\"s3://{default_bucket}/ChurnTransform\"\n",
    "    )\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name = \"ChurnTransform\",\n",
    "    transformer = transformer,\n",
    "    inputs = TransformInput(data = batch_data, content_type = \"text/csv\", join_source = \"Input\", split_type = \"Line\")\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ClarifyProcessingStep** step is defined in the variable named **step_clarify**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Processing.\n",
    "- **Processor:** This job uses SageMaker ClarifyProcessor. You can review the processor configuration in the variable named **clarify_processor**.\n",
    "- **Inputs:** The inputs are defined in the **data_input** and **congif_input** variables.\n",
    "- **Outputs:** The output is written to a folder under the default bucket. \n",
    "- **Depends On:**  Notice that this job cannot run until the configuration file required by Amazon SageMaker Clarify has been created by the **ChurnModelConfigFile**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-clarify-processing\n",
    "data_config = sagemaker.clarify.DataConfig(\n",
    "s3_data_input_path = f's3://{default_bucket}/output/train/train.csv',\n",
    "s3_output_path = bias_report_output_path,\n",
    "    label = 0,\n",
    "    headers = ['target','esent','eopenrate','eclickrate','avgorder','ordfreq','paperless','refill','doorstep','first_last_days_diff','created_first_days_diff','favday_Friday','favday_Monday','favday_Saturday','favday_Sunday','favday_Thursday','favday_Tuesday','favday_Wednesday','city_BLR','city_BOM','city_DEL','city_MAA'],\n",
    "    dataset_type = \"text/csv\",\n",
    ")\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = clarify_instance_type,\n",
    "    sagemaker_session = sagemaker_session,\n",
    ")\n",
    "\n",
    "config_input = ProcessingInput(\n",
    "    input_name = \"analysis_config\",\n",
    "    source=analysis_config_path,\n",
    "    destination = \"/opt/ml/processing/input/analysis_config\",\n",
    "    s3_data_type = \"S3Prefix\",\n",
    "    s3_input_mode = \"File\",\n",
    "    s3_compression_type = \"None\",\n",
    "    )\n",
    "\n",
    "data_input = ProcessingInput(\n",
    "    input_name = \"dataset\",\n",
    "    source = data_config.s3_data_input_path,\n",
    "    destination = \"/opt/ml/processing/input/data\",\n",
    "    s3_data_type = \"S3Prefix\",\n",
    "    s3_input_mode = \"File\",\n",
    "    s3_data_distribution_type = data_config.s3_data_distribution_type,\n",
    "    s3_compression_type = data_config.s3_compression_type,\n",
    ")\n",
    "\n",
    "result_output = ProcessingOutput( \n",
    "    source = \"/opt/ml/processing/output\",\n",
    "    destination = data_config.s3_output_path,\n",
    "    output_name = \"analysis_result\",\n",
    "    s3_upload_mode = \"EndOfJob\",\n",
    ")\n",
    "\n",
    "step_clarify = ProcessingStep(\n",
    "    name = \"ClarifyProcessingStep\",\n",
    "    processor = clarify_processor,\n",
    "    inputs = [data_input, config_input],\n",
    "    outputs = [result_output],\n",
    "    depends_on = [step_config_file.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RegisterChurnModel** step is defined in the variable named **step_register**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Register Model – Register jobs are defined using the class RegisterMode().\n",
    "- **Estimator:** The estimator is defined in the **xgbtrain** variable earlier in the cell.\n",
    "- **Model Data:** This is the model URI that is returned by **ChurnHyperParameterTuning**.\n",
    "- **Content Types:** text/csv\n",
    "- **Response Types** text/csv\n",
    "- **Inference Instance:** This is the instance type that will be used for inference processing.\n",
    "- **Transform Instance:** This is the instance type that will be used to process transformations.\n",
    "- **Model Package Group Name:** This is the name of the group that will store the group of model versions.\n",
    "- **Model Metrics:** This defines the location of the model metrics. Files included are the SageMaker Clarify bias report, SageMaker Clarify explainability report, and the model evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-model-registry\n",
    "model_statistics = MetricsSource(\n",
    "    s3_uri = \"s3://{}/output/evaluation/evaluation.json\".format(default_bucket),\n",
    "    content_type = \"application/json\"\n",
    "    )\n",
    "explainability = MetricsSource(\n",
    "    s3_uri = \"s3://{}/clarify-output/bias/analysis.json\".format(default_bucket),\n",
    "    content_type = \"application/json\"\n",
    "    )\n",
    "\n",
    "bias = MetricsSource(\n",
    "    s3_uri = \"s3://{}/clarify-output/bias/analysis.json\".format(default_bucket),\n",
    "    content_type = \"application/json\"\n",
    "    ) \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics = model_statistics,\n",
    "    explainability = explainability,\n",
    "    bias = bias\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name = \"RegisterChurnModel\",\n",
    "    estimator = xgb_train,\n",
    "    model_data = step_tuning.get_top_model_s3_uri(top_k = 0, s3_bucket = default_bucket, prefix = \"output\"),\n",
    "    content_types = [\"text/csv\"],\n",
    "    response_types = [\"text/csv\"],\n",
    "    inference_instances = [\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances = [\"ml.m5.large\"],\n",
    "    model_package_group_name = model_package_group_name,\n",
    "    model_metrics = model_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CheckAUCScoreChurnEvaluation** step is defined in the variable named **step_cond**. \n",
    "\n",
    "Step configuration includes the following:\n",
    "- **Type:** Condition – Condition jobs are defined using the class ConditionStep().\n",
    "- **Conditions:** This condition evaluates to True if the output from **ChurnEvalBestModel** is greater than 0.75.\n",
    "- **If Steps:** This is the list of steps that runs if the condition evaluates to True.\n",
    "- **Else Steps:** This is the list of steps that run if the condition evaluates to False. Notice that this list is empty, which means the pipeline stops processing if the condition is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cond_lte = ConditionGreaterThan(\n",
    "    left = JsonGet(\n",
    "        step = step_eval,\n",
    "        property_file = evaluation_report,\n",
    "        json_path = \"binary_classification_metrics.auc.value\"\n",
    "    ),\n",
    "    right = 0.75,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name = \"CheckAUCScoreChurnEvaluation\",\n",
    "    conditions = [cond_lte],\n",
    "    if_steps = [step_create_model, step_config_file, step_transform, step_clarify, step_register],\n",
    "    else_steps = [],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.10: Define the pipeline\n",
    "\n",
    "After you define the steps, you configure the pipeline in the variable named **pipeline**. Notice how steps that were previously defined are passed into the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #define pipeline function\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    role = None,\n",
    "    default_bucket = None,\n",
    "    model_package_group_name = \"ChurnModelPackageGroup\",\n",
    "    pipeline_name = \"ChurnModelPipeline\",\n",
    "    base_prefix = None,\n",
    "    custom_image_uri = None,\n",
    "    sklearn_processor_version = None\n",
    "    ):\n",
    "\n",
    "    #configure pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name = pipeline_name,\n",
    "        parameters = [\n",
    "            processing_instance_type,\n",
    "            processing_instance_count,\n",
    "            training_instance_type,\n",
    "            input_data,\n",
    "            batch_data,\n",
    "        ],\n",
    "        steps = [step_process, step_tuning, step_eval, step_cond],\n",
    "        sagemaker_session = sagemaker_session\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.11: Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #create pipeline using function\n",
    "pipeline = get_pipeline(\n",
    "  region = region,\n",
    "    role = role,\n",
    "    default_bucket = default_bucket,\n",
    "    model_package_group_name = model_package_group_name,\n",
    "    pipeline_name = pipeline_name,\n",
    "    custom_image_uri = clarify_image,\n",
    "    sklearn_processor_version = sklearn_processor_version\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.12: Update the pipeline to use the correct IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-iam-role\n",
    "pipeline.upsert(role_arn = role)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you get any sagemaker.workflow warnings after running the cell, you can safely ignore it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.13: Start the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start-pipeline\n",
    "RunPipeline = pipeline.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.14: Describe the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe-pipeline\n",
    "RunPipeline.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline takes about 35 minutes to run.\n",
    "\n",
    "While the pipeline is running, continue to the next task to explore the pipeline in the Amazon SageMaker Studio console."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor and approve the pipeline\n",
    "\n",
    "In this task, you explore the pipeline using the  Amazon SageMaker Studio console.\n",
    "\n",
    "### Task 2.15: Monitor the pipeline in SageMaker Studio\n",
    "\n",
    "1. Copy the **SagemakerStudioUrl** value to the left of these instructions.\n",
    "\n",
    "1. Open a new browser tab, and then paste the **SagemakerStudioUrl** value into the address bar.\n",
    "\n",
    "1. Press **Enter**.\n",
    "\n",
    "The browser displays the SageMaker Studio page.\n",
    "\n",
    "**Note:** You can view the JupyterLab workspace tab and SageMaker Studio tab side by side. This will allow you to have the directions displayed as you explore the pipeline steps.\n",
    "\n",
    "1. In the SageMaker Studio welcome popup window, choose **Skip Tour for now**.\n",
    "\n",
    "1. In the left panel navigation menu, choose **Pipelines**.\n",
    "\n",
    "SageMaker Studio opens the **Pipelines** page.\n",
    "\n",
    "1. Choose the pipeline named **ChurnModelSMPipeline**. \n",
    "\n",
    "SageMaker Studio opens the **ChurnModelSMPipeline** page and displays the execution details and status under the **Executions** tab.\n",
    "\n",
    "1. Choose the link of the execution under the **Name** column.\n",
    "\n",
    "The page displays the execution graph which shows all the steps of the execution. You can check if each step is executing, waiting to run, successfully completed, or generated an error.\n",
    "\n",
    "You can also choose any step in the graph to view the step details.\n",
    "\n",
    "1. On the graph, choose (double-click) the step named **ChurnModelProcess**. A new pane named **ChurnModelProcess** is displayed.\n",
    "\n",
    "1. In the **ChurnModelProcess** pane, review the tabs associated with this pipeline step: \n",
    "    - Choose the **Overview** tab.  This tab contains information about the step status. This tab also shows the different files that the pipeline step generates and where they are placed. The pipeline places all outputs in the SageMaker Studio default bucket.\n",
    "    - Choose the **Settings** tab. This tab contains helpful information about the parameters and files that the processing step uses. In the parameters section, there are details including the instance type and image that the job uses, dataset location, code location, and destinations for the different outputs that are generated. Scroll to the bottom of the pane to find the file inputs that were passed to the job.\n",
    "    - Choose the **Details** tab. This tab show details about the pipeline where this step is associated with. It also shows the step type under the **MetaData** section. This tab also shows the logs that the job generates. Having the logging available inside SageMaker Studio speeds up investigation and troubleshooting when a pipeline step fails to run successfully.\n",
    "\n",
    "### Task 2.16: Discover pipeline step details\n",
    "\n",
    "In the following steps, you choose the appropriate step from the execution graph to find information about a given pipeline step. If you need help finding the answers, correct responses or hints are included at the end of this notebook.\n",
    "\n",
    "1. For the step named **ChurnHyperParameterTuning**, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "    - What was the **Training Job** generated by this step?\n",
    "1. For the step named **ChurnEvalBestModel**, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "    - What is the name of the Python script that is used to evaluate the top model that was identified in the previous step?\n",
    "    - Where is this file located?\n",
    "    - Where were the results from this step written?\n",
    "1. For the step named **CheckAUCScoreChurnEvaluation**, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "    - What was the **Evaluation outcome**?\n",
    "1. For the step named **ChurnCreateModel**, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "    - Did this job generate any logs?\n",
    "1. For the step named **RegisterChurnModel-RegisterModel**, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "1. For the step named **ChurnTransform**, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "    - Did this job generate logs?\n",
    "    - Which files were inputs for this step?\n",
    "1. For the step named **ChurnModelConfigFile**, locate the following details: \n",
    "    - Which ProcessingInstanceType was used to run this job?\n",
    "    - What is the **Step Type** for this step?\n",
    "1. For the step named **ClarifyProcessingStep**, locate the following details:\n",
    "    - What was the file output from this step?\n",
    "    - Where was the output written?\n",
    "\n",
    "### 2.17: Approve the model in the pipeline\n",
    "\n",
    "After the pipeline has finished running, view the model that the pipeline created.\n",
    "\n",
    "1. In the SageMaker Studio, from the left navigation panel, choose **Models**.\n",
    "\n",
    "1. In the **Models** page, choose the **ChurnModelPackageGroup** link.\n",
    "\n",
    "1. Choose the version link listed in the table.\n",
    "\n",
    "    SageMaker Studio displays the overview page for the version chosen.\n",
    "\n",
    "    - Notice that the model deploy status is **Pending Approval**.\n",
    "\n",
    "    Additional details about the pipeline are found in the **Activity:** and **Details** tabs.\n",
    "\n",
    "1. Approve the model. This process is designed for manual review before approving the model. However, it is possible to automate the model approval within the pipeline:\n",
    "    - Choose **Overview** tab and from **Deploy** tile.\n",
    "    - Choose **Approved**.\n",
    "    - Choose **Save**.\n",
    "\n",
    "### Task 2.18: View the pipeline steps using the Amazon SageMaker Python SDK\n",
    "\n",
    "In addition to using the SageMaker Studio UI to view pipeline details, you can also use the Amazon SageMaker Python SDK commands. For example, the following Boto3 command returns a list of the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list-steps\n",
    "RunPipeline.list_steps()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the artifacts\n",
    "\n",
    "### Task 2.19: Review the Amazon SageMaker Clarify analysis and locate pipeline artifacts in the default SageMaker S3 bucket\n",
    "\n",
    "Artifacts generated by the pipeline are saved in the default SageMaker Amazon S3 bucket. You could navigate to Amazon S3 via the AWS Management Console to review these artifacts. Instead, for this lab the Amazon S3 browser extension for JupyterLab workspace has been installed. This permits access to objects stored in Amazon S3, from directly within JupyterLab workspace.\n",
    "\n",
    "1. In JupyterLab workspace, choose the **Object Storage Browser** icon, located on the left panel.\n",
    "\n",
    "1. Choose the bucket with a name that begins with **sagemaker-** and the AWS Region; for example **sagemaker-us-west-2-123456789**.\n",
    "\n",
    "1. Navigate to the **clarify-output/bias/** subfolder.\n",
    "\n",
    "1. Open the **report.ipynb** notebook file.\n",
    "\n",
    "1. In the **Select Kernel** window, for **Select kernel for: \"report.ipynb\"**, select **Python 3 (ipykernel)**, then choose **Select**.\n",
    "\n",
    "1. Explore the output of the Amazon SageMaker Clarify analysis.\n",
    "\n",
    "Any other artifacts within the default SageMaker S3 bucket can be explored."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Build and review the lineage for the pipeline\n",
    "\n",
    "You learned how to use SageMaker Clarify to help explain how a model makes predictions and understand the potential bias of a model. You can also use SageMaker Clarify to discover the steps that are used to generate the model, which are often needed for model auditing. In this task you take advantage of the MLLineageHelper module to build the lineage of the current pipeline run. Refer to [MLLineageHelper](https://github.com/aws-samples/ml-lineage-helper) for more information about ML Lineage Helper.\n",
    "\n",
    "Amazon SageMaker ML Lineage Tracking creates and stores information about the steps of an ML workflow from data preparation to model deployment. With the tracking information, you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.21: Setting up the session and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-variables\n",
    "fs_query = feature_group.athena_query()\n",
    "fs_table = fs_query.table_name\n",
    "query_string = 'SELECT * FROM \"'+fs_table+'\"'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.22: Show values that will be used to build the model's lineage\n",
    "\n",
    "Configurations include the following:\n",
    "- **query_string:** This is the SageMaker Feature Store query that will be passed to the MLLineageHelper module.\n",
    "- **model_ref:** This is the name of the model that is being evaluated.\n",
    "- **processing_job:** This is the name of the processing job that generated the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-values\n",
    "print ('query_string:',query_string)\n",
    "\n",
    "model_ref = sagemaker_client.list_models(SortBy = 'CreationTime', SortOrder = 'Descending')['Models'][0]['ModelName']\n",
    "print ('model_ref:',model_ref)\n",
    "\n",
    "processing_job = sagemaker_client.list_processing_jobs(SortBy = 'CreationTime', SortOrder = 'Descending', NameContains = 'ChurnModelProcess')['ProcessingJobSummaries'][0]['ProcessingJobName']\n",
    "print ('processing_job:',processing_job)\n",
    "\n",
    "processing_job_description = sagemaker_client.describe_processing_job(\n",
    "    ProcessingJobName = processing_job\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.23: Describe the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe-processing-job\n",
    "processing_job_description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.24: Show the name of the training job used to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-training-job\n",
    "training_job_name  =  sagemaker_client.list_training_jobs(SortBy = 'CreationTime', SortOrder = 'Descending')['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "print (training_job_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.25: Build the lineage for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive the following error, run the cell again.\n",
    "- **ClientError: An error occurred (ThrottlingException) when calling the UpdateArtifact operation (reached max retries: 4): Rate exceeded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build-lineage\n",
    "ml_lineage = MLLineageHelper()\n",
    "lineage = ml_lineage.create_ml_lineage(training_job_name, model_name = model_ref,\n",
    "                                       query = query_string, sagemaker_processing_job_description = processing_job_description,\n",
    "                                       feature_group_names = [feature_group_name])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.26: Limit the lineage to include only the current trial and feature group\n",
    "\n",
    "A pipeline can run multiple times. To ensure that you are retrieving details from the most recent training job run, filter the lineage call using the name of the current trial and the feature group that the trial uses. \n",
    "\n",
    "After you run this cell, the steps used to create the model, the order in which the steps ran, and which jobs contributed to other jobs in the pipeline are displayed as a table. This same information is also written to a file named **lineage_FS.csv**. You can download this file to save the output and share it with other team members, such as auditors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit-lineage\n",
    "trial_name = RunPipeline.describe()['PipelineExperimentConfig']['TrialName']\n",
    "pat = str(trial_name)+'|'+'fg-FG'\n",
    "df1 = lineage[lineage.apply(lambda x: any(x.str.contains(pat)),axis = 1)]\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "df1.to_csv('lineage_FS.csv') \n",
    "df1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.27: Generate a visualization of the model's lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize-lineage\n",
    "plt.figure(3, figsize = (20, 14))\n",
    "graph = nx.DiGraph()\n",
    "graph.add_edges_from([(each[0], each[2]) for each in df1.values])\n",
    "fig, ax = plt.subplots()\n",
    "nx.draw_networkx(\n",
    "    graph,\n",
    "    node_size = 300,\n",
    "    node_color = \"orange\",\n",
    "    alpha = 0.65,\n",
    "    font_size = 8,\n",
    "    pos = nx.spring_layout(graph)\n",
    ")\n",
    "ax.set_facecolor('deepskyblue')\n",
    "ax.axis('off')\n",
    "fig.set_facecolor('deepskyblue')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.28: Remove the pipeline\n",
    "\n",
    "To delete the pipeline, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete-pipeline\n",
    "response = sagemaker_client.delete_pipeline(PipelineName = 'ChurnModelSMPipeline')\n",
    "print (response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "Congratulations! You have used Amazon SageMaker Pipelines to automate the creation and registry of a model. You learned how to drill down into each pipeline step to identify associated parameters, files, and logs. You know how to identify the assets that the pipeline used to generate the model, how to find the model in the model registry, and how to find and view the explainability and bias reports that a pipeline can generate.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints and answers for task 2.16: Discover pipeline step details\n",
    "General hint: **Step type** is found on the **Metadata** section in the **Details** tab.\n",
    "\n",
    "1. For the step named **ChurnHyperParameterTuning**, locate the following details:\n",
    "    - What is the **Step Type** for this step?</br>\n",
    "    **Hint:** This information is found on the **Details** tab under the **Metadata** section.</br>\n",
    "    **Answer:** Tuning</br>\n",
    "    - What was the **Training Job** generated by this step? </br>\n",
    "    **Hint:** This information is found on the **Overview** tab.</br>\n",
    "    **Answer:** The model name is generated and will be different for each student. The name should be similar to this example: 056vhzs2vkxc-ChurnHy-TCAtUr16oV-001-17d5bd01\n",
    "1. For the **ChurnEvalBestModel** step, locate the following details:\n",
    "    - What is the **Step Type** for this step?\n",
    "    **Answer:** Processing</br>\n",
    "    - What is the name of the Python script that is used to evaluate the top model that was identified in the previous step?</br>\n",
    "    **Hint:** This information is found on the **Settings** tab under the **Files** section.</br>\n",
    "    **Answer:** evaluate.py</br>\n",
    "    - Where is this file located?</br>\n",
    "    **Hint:** This information is found on the **Settings** tab under the **Files** section.</br>\n",
    "    **Answer:** The file resides in an S3 Bucket. The path is similar to this example: s3://sagemaker-us-west-2-1234567890/input/code/evaluate.py</br>\n",
    "    - Where were the results from this step written?</br>\n",
    "    **Hint:** This information is found on the **Settings** tab under the **Parameters** section, and then under **evaluation**.</br></br>\n",
    "    **Answer:** The results of the evaluation were written to an S3 bucket. The path to the file should be similar to the following example: s3://sagemaker-us-west-2-1234567890/output/evaluation</br>\n",
    "1. For the **CheckAUCScoreChurnEvaluation** step, locate the following details:\n",
    "    - What is the **Step Type** for this step?</br>\n",
    "    **Answer:** Condition</br>\n",
    "    - What was the **Evaluation outcome**?</br>\n",
    "    **Hint:** This information is found on the **Details** tab under the **Metadata** section.</br>\n",
    "    **Answer:** True\n",
    "1. For the **ChurnCreateModel** step, locate the following details:\n",
    "    - What is the **Step Type** for this step?</br>\n",
    "    **Answer:** Model</br>\n",
    "    - Did this job generate any logs?</br>\n",
    "    **Answer:** No\n",
    "1. For the **RegisterChurnModel-RegisterModel** step, locate the following details:\n",
    "    - What is the **Step Type** for this step?</br>\n",
    "    **Answer:** RegisterModel</br>\n",
    "1. For the **ChurnTransform** step, locate the following details:\n",
    "    - What is the **Step Type** for this step?</br>\n",
    "    **Answer:** Transform</br>\n",
    "    - Did this job generate logs?</br>\n",
    "    **Answer:** Yes</br>\n",
    "    - Which files were inputs for this step?</br>\n",
    "    **Hint:** This information is found on the **Settings** tab under the **Files** section. You might need to scroll to the bottom of the pane to find the file names.</br>\n",
    "    **Answer:** model.tar.gz, sagemaker-xgboost:1.5-1-cpu-py3, batch.csv\n",
    "1. For the **ChurnModelConfigFile** step, locate the following details: \n",
    "    - Which ProcessingInstanceType was used to run this job?</br>\n",
    "    **Hint:** This information is found on the **Settings** tab.</br>\n",
    "    **Answer:** ml.m5.xlarge\n",
    "    - What is the **Step Type** for this step?</br>\n",
    "    **Answer:** Processing\n",
    "1. For the **ClarifyProcessingStep**, locate the following details:\n",
    "    - What was the file output from this step?\n",
    "    **Hint:** This information is found on the **Settings** tab under the **Files** section.</br>\n",
    "    **Answer:** The output was bias data.\n",
    "    - Where was the output written?\n",
    "    **Answer:** The output was written to an S3 Bucket. The path should be similar to this example: s3://sagemaker-us-west-2-1234567890/clarify-output/bias"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
